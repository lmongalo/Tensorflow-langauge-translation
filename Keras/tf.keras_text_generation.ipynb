{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow >= 1.9 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: Once you enable eager execution, it cannot be disabled. \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = ('Zulu.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = unidecode.unidecode(open(path_to_file).read())\n",
    "# length of text is the number of characters in it\n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 100\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f+1:f+1+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.batch_sz = batch_size\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True, \n",
    "                                     recurrent_activation='sigmoid', \n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True, \n",
    "                                     recurrent_activation='sigmoid', \n",
    "                                     recurrent_initializer='glorot_uniform'\n",
    "    \n",
    "      self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # output shape == (batch_size, max_length, hidden_size) \n",
    "    # states shape == (batch_size, hidden_size)\n",
    "\n",
    "    # states variable to preserve the state of the model\n",
    "    # this will be used to pass at every step to the model while training\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "\n",
    "\n",
    "    # reshaping the output so that we can pass it to the Dense layer\n",
    "    # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # The dense layer will output predictions for every time_steps(max_length)\n",
    "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions, hidden = model(inp, hidden)\n",
    "              \n",
    "              # reshaping the target because that's how the \n",
    "              # loss function expects it\n",
    "              target = tf.reshape(target, (-1,))\n",
    "              loss = loss_function(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                            batch,\n",
    "                                                            loss))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation step(generating text using the model learned)\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'Q'\n",
    "# converting our start string to numbers(vectorizing!) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# empty string to store our results\n",
    "text_generated = ''\n",
    "\n",
    "# low temperatures results in more predictable text.\n",
    "# higher temperatures results in more surprising text\n",
    "# experiment to find the best setting\n",
    "temperature = 1.0\n",
    "\n",
    "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated += idx2char[predicted_id]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
